{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reproduce_CO_v3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"FJONlHfivgMb","colab_type":"code","outputId":"d2813a56-b4a0-4874-9aa5-441dcd67c18f","executionInfo":{"status":"ok","timestamp":1554073489808,"user_tz":240,"elapsed":498,"user":{"displayName":"Qiang Ma","photoUrl":"https://lh6.googleusercontent.com/-xvqSKhxJSkY/AAAAAAAAAAI/AAAAAAAAAAs/aWtprnac56k/s64/photo.jpg","userId":"05776109497485911566"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"d-Wlgv92vuWl","colab_type":"text"},"cell_type":"markdown","source":["# Generate Data"]},{"metadata":{"id":"trnXgxO8vt-7","colab_type":"code","colab":{}},"cell_type":"code","source":["import requests\n","from tqdm import tqdm\n","from torch.utils.data import Dataset\n","from torch.autograd import Variable\n","import torch\n","import os\n","import numpy as np\n","import re\n","import zipfile\n","import itertools\n","from collections import namedtuple\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XR6Myzfqv3q7","colab_type":"code","colab":{}},"cell_type":"code","source":["# Training Data\n","class TSPDataset(Dataset):\n","    \n","    def __init__(self, dataset_fname=None, train=False, size=50, num_samples=100000, random_seed=1111):\n","        super(TSPDataset, self).__init__()\n","        \n","        torch.manual_seed(random_seed)\n","\n","        self.data_set = []\n","        \n","        # randomly sample points uniformly from [0, 1]\n","        for l in tqdm(range(num_samples)):\n","            x = torch.FloatTensor(2, size).uniform_(0, 1)\n","            #x = torch.cat([start, x], 1)\n","            self.data_set.append(x)\n","\n","        self.size = len(self.data_set)\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def __getitem__(self, idx):\n","        return self.data_set[idx]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iE1zTJ6Cv8uj","colab_type":"text"},"cell_type":"markdown","source":["## DataLoader"]},{"metadata":{"id":"QRn9F8c0v5cj","colab_type":"code","colab":{}},"cell_type":"code","source":["import pprint as pp\n","import numpy as np\n","\n","import torch\n","from torch.utils.data import DataLoader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EKO_ilm9v8B_","colab_type":"code","colab":{}},"cell_type":"code","source":["input_dim = 2\n","size = 50\n","train_size = 1280000\n","val_size = 1000\n","batch_size = 128"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OnZurVCDwAkf","colab_type":"code","outputId":"04da8921-cd3e-4d3d-817c-e13266f8daed","executionInfo":{"status":"ok","timestamp":1554073514218,"user_tz":240,"elapsed":20025,"user":{"displayName":"Qiang Ma","photoUrl":"https://lh6.googleusercontent.com/-xvqSKhxJSkY/AAAAAAAAAAI/AAAAAAAAAAs/aWtprnac56k/s64/photo.jpg","userId":"05776109497485911566"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["training_dataset = TSPDataset(train=True, size=size,\n","     num_samples=train_size)\n","\n","val_dataset = TSPDataset(train=True, size=size,\n","        num_samples=val_size)\n","\n","train_dataloader = DataLoader(training_dataset, batch_size=batch_size,\n","    shuffle=True, num_workers=4)\n","\n","val_dataloader = DataLoader(val_dataset, batch_size=int(batch_size/2), shuffle=True, num_workers=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 1280000/1280000 [00:08<00:00, 158112.40it/s]\n","100%|██████████| 1000/1000 [00:00<00:00, 127808.88it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"EGdCTITmwsMs","colab_type":"text"},"cell_type":"markdown","source":["# Neural Net Model"]},{"metadata":{"id":"HcbzYnHNws5t","colab_type":"text"},"cell_type":"markdown","source":["## Encoder"]},{"metadata":{"id":"gzMgAKZpwB9y","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.autograd as autograd\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import math\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uoejC9qZwwNT","colab_type":"code","colab":{}},"cell_type":"code","source":["class Encoder(nn.Module):\n","    \"\"\"Maps a graph represented as an input sequence\n","    to a hidden vector\"\"\"\n","    def __init__(self, input_dim, hidden_dim, use_cuda):\n","        super(Encoder, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.lstm = nn.LSTM(input_dim, hidden_dim)\n","        self.use_cuda = use_cuda\n","        self.enc_init_state = self.init_hidden(hidden_dim)\n","\n","    def forward(self, x, hidden):\n","        output, hidden = self.lstm(x, hidden)\n","        return output, hidden\n","    \n","    def init_hidden(self, hidden_dim):\n","        \"\"\"Trainable initial hidden state\"\"\"\n","        enc_init_hx = Variable(torch.zeros(hidden_dim), requires_grad=False)\n","        if self.use_cuda:\n","            enc_init_hx = enc_init_hx.cuda()\n","\n","        #enc_init_hx.data.uniform_(-(1. / math.sqrt(hidden_dim)),\n","        #        1. / math.sqrt(hidden_dim))\n","\n","        enc_init_cx = Variable(torch.zeros(hidden_dim), requires_grad=False)\n","        if self.use_cuda:\n","            enc_init_cx = enc_init_cx.cuda()\n","\n","        #enc_init_cx = nn.Parameter(enc_init_cx)\n","        #enc_init_cx.data.uniform_(-(1. / math.sqrt(hidden_dim)),\n","        #        1. / math.sqrt(hidden_dim))\n","        return (enc_init_hx, enc_init_cx)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8a2Tgp9GwycP","colab_type":"text"},"cell_type":"markdown","source":["## Attention"]},{"metadata":{"id":"E7SbZzRswx_N","colab_type":"code","colab":{}},"cell_type":"code","source":["class Attention(nn.Module):\n","    \"\"\"A generic attention module for a decoder in seq2seq\"\"\"\n","    def __init__(self, dim, use_tanh=False, C=10, use_cuda=True):\n","        super(Attention, self).__init__()\n","        self.use_tanh = use_tanh\n","        # query matrix W_q\n","        self.project_query = nn.Linear(dim, dim)\n","        # refence matrix W_ref\n","        self.project_ref = nn.Conv1d(dim, dim, 1, 1)\n","        self.C = C  # tanh exploration\n","        self.tanh = nn.Tanh()\n","        \n","        v = torch.FloatTensor(dim)\n","        if use_cuda:\n","            v = v.cuda()  \n","        self.v = nn.Parameter(v)\n","        self.v.data.uniform_(-(1. / math.sqrt(dim)) , 1. / math.sqrt(dim))\n","        \n","    def forward(self, query, ref):\n","        \"\"\"\n","        Args: \n","            query: is the hidden state of the decoder at the current\n","                time step. batch x dim\n","            ref: the set of hidden states from the encoder. \n","                sourceL x batch x hidden_dim\n","        \"\"\"\n","        # ref is now [batch_size x hidden_dim x sourceL]\n","        ref = ref.permute(1, 2, 0)\n","        q = self.project_query(query).unsqueeze(2)  # batch x dim x 1\n","        e = self.project_ref(ref)  # batch_size x hidden_dim x sourceL \n","        # expand the query by sourceL\n","        # batch x dim x sourceL\n","        expanded_q = q.repeat(1, 1, e.size(2)) \n","        # batch x 1 x hidden_dim\n","        v_view = self.v.unsqueeze(0).expand(\n","                expanded_q.size(0), len(self.v)).unsqueeze(1)\n","        # [batch_size x 1 x hidden_dim] * [batch_size x hidden_dim x sourceL]\n","        u = torch.bmm(v_view, self.tanh(expanded_q + e)).squeeze(1)\n","        if self.use_tanh:\n","            logits = self.C * self.tanh(u)\n","        else:\n","            logits = u  \n","            \n","        # e is projected reference, logits is u\n","        # ref is used to compute glimpse: G = ref * softmax(u)\n","        \n","        # the logits has the same length with the number of cities\n","        # so softmax(logits) is exact the probability of each city\n","        return e, logits\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sVm9-az2w5-k","colab_type":"text"},"cell_type":"markdown","source":["## Decoder"]},{"metadata":{"id":"X4zooEF2w3s-","colab_type":"code","colab":{}},"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, \n","            embedding_dim,\n","            hidden_dim,\n","            max_length,\n","            tanh_exploration,\n","            terminating_symbol,\n","            use_tanh,\n","            decode_type,\n","            n_glimpses=1,\n","            beam_size=0,\n","            use_cuda=True):\n","        super(Decoder, self).__init__()\n","        \n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_glimpses = n_glimpses\n","        self.max_length = max_length\n","        self.terminating_symbol = terminating_symbol \n","        self.decode_type = decode_type\n","        self.beam_size = beam_size\n","        self.use_cuda = use_cuda\n","\n","        self.input_weights = nn.Linear(embedding_dim, 4 * hidden_dim)\n","        self.hidden_weights = nn.Linear(hidden_dim, 4 * hidden_dim)\n","\n","        self.pointer = Attention(hidden_dim, use_tanh=use_tanh, C=tanh_exploration, use_cuda=self.use_cuda)\n","        self.glimpse = Attention(hidden_dim, use_tanh=False, use_cuda=self.use_cuda)\n","        self.sm = nn.Softmax(dim=-1)\n","\n","    def apply_mask_to_logits(self, step, logits, mask, prev_idxs):    \n","        if mask is None:\n","            mask = torch.zeros(logits.size()).byte()\n","            if self.use_cuda:\n","                mask = mask.cuda()\n","    \n","        maskk = mask.clone()\n","\n","        # to prevent them from being reselected. \n","        # Or, allow re-selection and penalize in the objective function\n","        if prev_idxs is not None:\n","            # set most recently selected idx values to 1\n","            maskk[[x for x in range(logits.size(0))],\n","                    prev_idxs.data] = 1\n","            # just implement the equation (8) in the paper\n","            logits[maskk] = -np.inf\n","        return logits, maskk\n","\n","    def forward(self, decoder_input, embedded_inputs, hidden, context):\n","        \"\"\"\n","        Args:\n","            decoder_input: The initial input to the decoder\n","                size is [batch_size x embedding_dim]. Trainable parameter.\n","            embedded_inputs: [sourceL x batch_size x embedding_dim]\n","            hidden: the prev hidden state, size is [batch_size x hidden_dim]. \n","                Initially this is set to (enc_h[-1], enc_c[-1])\n","            context: encoder outputs, [sourceL x batch_size x hidden_dim] \n","        \"\"\"\n","        def recurrence(x, hidden, logit_mask, prev_idxs, step):\n","            \n","            hx, cx = hidden  # batch_size x hidden_dim\n","            \n","            gates = self.input_weights(x) + self.hidden_weights(hx)\n","            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n","\n","            ingate = torch.sigmoid(ingate)\n","            forgetgate = torch.sigmoid(forgetgate)\n","            cellgate = torch.tanh(cellgate)\n","            outgate = torch.sigmoid(outgate)\n","\n","            cy = (forgetgate * cx) + (ingate * cellgate)\n","            hy = outgate * torch.tanh(cy)  # batch_size x hidden_dim\n","            \n","            g_l = hy\n","            for i in range(self.n_glimpses):\n","                ref, logits = self.glimpse(g_l, context)\n","                logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n","                # [batch_size x h_dim x sourceL] * [batch_size x sourceL x 1] = \n","                # [batch_size x h_dim x 1]\n","                g_l = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2) \n","            _, logits = self.pointer(g_l, context)\n","            \n","            # update the mask\n","            logits, logit_mask = self.apply_mask_to_logits(step, logits, logit_mask, prev_idxs)\n","            probs = self.sm(logits)\n","            return hy, cy, probs, logit_mask\n","    \n","        batch_size = context.size(1)\n","        outputs = []\n","        selections = []\n","        steps = range(self.max_length)  # or until terminating symbol ?\n","        inps = []\n","        idxs = None\n","        mask = None\n","       \n","        if self.decode_type == \"stochastic\":\n","            # for loop to get all city coordinates\n","            for i in steps:\n","                # the first decoder_input is just a trainable parameters\n","                # the next decoder_input is the embedded_inputs[idxs]\n","                hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n","                # hidden contains two tensors with size [batch_size x hidden_dim]\n","                # and the (hy, cy) is the next (hx, cx)\n","                # the first hidden comes from the encoder\n","                hidden = (hx, cx)\n","                # select the next inputs for the decoder [batch_size x hidden_dim]\n","                decoder_input, idxs = self.decode_stochastic(\n","                    probs,\n","                    embedded_inputs,\n","                    selections)\n","                inps.append(decoder_input) \n","                # use outs to point to next object\n","                outputs.append(probs)\n","                selections.append(idxs)\n","            return (outputs, selections), hidden\n","                \n","        elif self.decode_type == \"greedy\":\n","            for i in steps:\n","                hx, cx, probs, mask = recurrence(decoder_input, hidden, mask, idxs, i)\n","                hidden = (hx, cx)\n","                decoder_input, idxs = self.decode_greedy(\n","                    probs,\n","                    embedded_inputs,\n","                    selections)\n","                inps.append(decoder_input) \n","                outputs.append(probs)\n","                selections.append(idxs)\n","                \n","            return (outputs, selections), hidden\n","        \n","    def decode_stochastic(self, probs, embedded_inputs, selections):\n","        \"\"\"\n","        Return the next input for the decoder by selecting the \n","        input corresponding to the max output\n","        Args: \n","            probs: [batch_size x sourceL]\n","            embedded_inputs: [sourceL x batch_size x embedding_dim]\n","            selections: list of all of the previously selected indices during decoding\n","       Returns:\n","            Tensor of size [batch_size x sourceL] containing the embeddings\n","            from the inputs corresponding to the [batch_size] indices\n","            selected for this iteration of the decoding, as well as the \n","            corresponding indicies\n","        \"\"\"\n","        batch_size = probs.size(0)\n","        # idxs is [batch_size]\n","        # idxs = probs.multinomial().squeeze(1)\n","        # idxs is the sampled city index\n","        c = torch.distributions.Categorical(probs)\n","        \n","        idxs = c.sample()\n","        \n","        # due to race conditions, might need to resample here\n","        for old_idxs in selections:\n","            # compare new idxs\n","            # elementwise with the previous idxs. If any matches,\n","            # then need to resample\n","            if old_idxs.eq(idxs).data.any():\n","                print(' [!] resampling due to race condition')\n","                idxs = probs.multinomial().squeeze(1)\n","                break\n","        \n","        # embedded inputs are the embedded city coordinates, the next index is idxs,\n","        # so the next embedded input is embedded_inputs[idex]\n","        sels = embedded_inputs[idxs.data, [i for i in range(batch_size)], :] \n","        return sels, idxs\n","    \n","    def decode_greedy(self, probs, embedded_inputs, selections):\n","\n","        batch_size = probs.size(0)\n","        # idxs is [batch_size]\n","        # idxs is the greedy city index\n","        _, idxs = torch.max(probs,1)\n","        \n","        # embedded inputs are the embedded city coordinates, the next index is idxs,\n","        # so the next embedded input is embedded_inputs[idex]\n","        sels = embedded_inputs[idxs.data, [i for i in range(batch_size)], :] \n","        return sels, idxs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_Mp6qyxhxIR2","colab_type":"text"},"cell_type":"markdown","source":["## Pointer Network"]},{"metadata":{"id":"QGRfBUq0w8SR","colab_type":"code","colab":{}},"cell_type":"code","source":["class PointerNetwork(nn.Module):\n","    \"\"\"\n","    The pointer network, which is the core seq2seq model\n","    \"\"\"\n","    def __init__(self, \n","            embedding_dim,\n","            hidden_dim,\n","            max_decoding_len,\n","            terminating_symbol,\n","            n_glimpses,\n","            tanh_exploration,\n","            use_tanh,\n","            beam_size,\n","            use_cuda):\n","        super(PointerNetwork, self).__init__()\n","\n","        self.encoder = Encoder(\n","                embedding_dim,\n","                hidden_dim,\n","                use_cuda)\n","\n","        self.decoder = Decoder(\n","                embedding_dim,\n","                hidden_dim,\n","                max_length=max_decoding_len,\n","                tanh_exploration=tanh_exploration,\n","                use_tanh=use_tanh,\n","                terminating_symbol=terminating_symbol,\n","                decode_type=\"stochastic\",\n","                n_glimpses=n_glimpses,\n","                beam_size=beam_size,\n","                use_cuda=use_cuda)\n","\n","        # Trainable initial hidden states\n","        dec_in_0 = torch.FloatTensor(embedding_dim)\n","        if use_cuda:\n","            dec_in_0 = dec_in_0.cuda()\n","\n","        self.decoder_in_0 = nn.Parameter(dec_in_0)\n","        self.decoder_in_0.data.uniform_(-(1. / math.sqrt(embedding_dim)),\n","                1. / math.sqrt(embedding_dim))\n","            \n","    def forward(self, inputs):\n","        \"\"\" Propagate inputs through the network\n","        Args: \n","            inputs: [sourceL x batch_size x embedding_dim]\n","        \"\"\"\n","        \n","        (encoder_hx, encoder_cx) = self.encoder.enc_init_state\n","        encoder_hx = encoder_hx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)       \n","        encoder_cx = encoder_cx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)       \n","        \n","        # encoder forward pass\n","        enc_h, (enc_h_t, enc_c_t) = self.encoder(inputs, (encoder_hx, encoder_cx))\n","\n","        dec_init_state = (enc_h_t[-1], enc_c_t[-1])\n","    \n","        '''\n","        The first decoder_input is just the random trainable parameters\n","        \n","        The inputs are the embedded x (city coordinates)\n","        \n","        The initial decoder hidden states are the first encoded hidden variable\n","        \n","        The encoded latent variable is passed in decoder as the context, also\n","        as the reference in the attention\n","        '''\n","    \n","        # repeat decoder_in_0 across batch\n","        decoder_input = self.decoder_in_0.unsqueeze(0).repeat(inputs.size(1), 1)\n","        \n","        (pointer_probs, input_idxs), dec_hidden_t = self.decoder(decoder_input,\n","                inputs,\n","                dec_init_state,\n","                enc_h)\n","        \n","        return pointer_probs, input_idxs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BUXUKmjGxLlZ","colab_type":"text"},"cell_type":"markdown","source":["## Critic Network"]},{"metadata":{"id":"GN9PmTA8xJ0N","colab_type":"code","colab":{}},"cell_type":"code","source":["class CriticNetwork(nn.Module):\n","    \"\"\"Useful as a baseline in REINFORCE updates\"\"\"\n","    def __init__(self,\n","            embedding_dim,\n","            hidden_dim,\n","            n_process_block_iters,\n","            tanh_exploration,\n","            use_tanh,\n","            use_cuda):\n","        super(CriticNetwork, self).__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.n_process_block_iters = n_process_block_iters\n","\n","        self.encoder = Encoder(\n","                embedding_dim,\n","                hidden_dim,\n","                use_cuda)\n","        \n","        self.process_block = Attention(hidden_dim,\n","                use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n","        self.sm = nn.Softmax(dim=-1)\n","        self.decoder = nn.Sequential(\n","                nn.Linear(hidden_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, 1)\n","        )\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        Args:\n","            inputs: [embedding_dim x batch_size x sourceL] of embedded inputs\n","        \"\"\"\n","         \n","        (encoder_hx, encoder_cx) = self.encoder.enc_init_state\n","        encoder_hx = encoder_hx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)\n","        encoder_cx = encoder_cx.unsqueeze(0).repeat(inputs.size(1), 1).unsqueeze(0)       \n","        \n","        # encoder forward pass\n","        enc_outputs, (enc_h_t, enc_c_t) = self.encoder(inputs, (encoder_hx, encoder_cx))\n","        \n","        # grab the hidden state and process it via the process block \n","        process_block_state = enc_h_t[-1]\n","        for i in range(self.n_process_block_iters):\n","            ref, logits = self.process_block(process_block_state, enc_outputs)\n","            process_block_state = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2)\n","        # produce the final scalar output\n","        out = self.decoder(process_block_state)\n","        return out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SG_tYpghxPep","colab_type":"text"},"cell_type":"markdown","source":["## Final Model"]},{"metadata":{"id":"T5Em4wX8xNBf","colab_type":"code","colab":{}},"cell_type":"code","source":["class NeuralCombOptRL(nn.Module):\n","    \"\"\"\n","    This module contains the PointerNetwork (actor) and\n","    CriticNetwork (critic). It requires\n","    an application-specific reward function\n","    \"\"\"\n","    def __init__(self, \n","            input_dim,\n","            embedding_dim,\n","            hidden_dim,\n","            max_decoding_len,\n","            terminating_symbol,\n","            n_glimpses,\n","            n_process_block_iters,\n","            tanh_exploration,\n","            use_tanh,\n","            beam_size,\n","            objective_fn,\n","            is_train,\n","            use_cuda):\n","        super(NeuralCombOptRL, self).__init__()\n","        self.objective_fn = objective_fn\n","        self.input_dim = input_dim\n","        self.is_train = is_train\n","        self.use_cuda = use_cuda\n","\n","        \n","        self.actor_net = PointerNetwork(\n","                embedding_dim,\n","                hidden_dim,\n","                max_decoding_len,\n","                terminating_symbol,\n","                n_glimpses,\n","                tanh_exploration,\n","                use_tanh,\n","                beam_size,\n","                use_cuda)\n","        \n","        #self.critic_net = CriticNetwork(\n","        #        embedding_dim,\n","        #        hidden_dim,\n","        #        n_process_block_iters,\n","        #        tanh_exploration,\n","        #        False,\n","        #        use_cuda)\n","       \n","        embedding_ = torch.FloatTensor(input_dim,\n","            embedding_dim)\n","        if self.use_cuda: \n","            embedding_ = embedding_.cuda()\n","        self.embedding = nn.Parameter(embedding_) \n","        self.embedding.data.uniform_(-(1. / math.sqrt(embedding_dim)),\n","            1. / math.sqrt(embedding_dim))\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        Args:\n","            inputs: [batch_size, input_dim, sourceL]\n","        \"\"\"\n","        batch_size = inputs.size(0)\n","        input_dim = inputs.size(1)\n","        sourceL = inputs.size(2)\n","\n","        # repeat embeddings across batch_size\n","        # result is [batch_size x input_dim x embedding_dim]\n","        embedding = self.embedding.repeat(batch_size, 1, 1)  \n","        embedded_inputs = []\n","        # result is [batch_size, 1, input_dim, sourceL] \n","        ips = inputs.unsqueeze(1)\n","        \n","        for i in range(sourceL):\n","            # [batch_size x 1 x input_dim] * [batch_size x input_dim x embedding_dim]\n","            # result is [batch_size, embedding_dim]\n","            embedded_inputs.append(torch.bmm(\n","                ips[:, :, :, i].float(),\n","                embedding).squeeze(1))\n","\n","        # Result is [sourceL x batch_size x embedding_dim]\n","        embedded_inputs = torch.cat(embedded_inputs).view(\n","                sourceL,\n","                batch_size,\n","                embedding.size(2))\n","\n","        # query the actor net for the input indices \n","        # making up the output, and the pointer attn \n","        probs_, action_idxs = self.actor_net(embedded_inputs)\n","       \n","        # Select the actions (inputs pointed to \n","        # by the pointer net) and the corresponding\n","        # logits\n","        # should be size [batch_size x \n","        actions = []\n","        # inputs is [batch_size, input_dim, sourceL]\n","        inputs_ = inputs.transpose(1, 2)\n","        # inputs_ is [batch_size, sourceL, input_dim]\n","        for action_id in action_idxs:\n","            actions.append(inputs_[[x for x in range(batch_size)], action_id.data, :])\n","\n","        if self.is_train:\n","            # probs_ is a list of len sourceL of [batch_size x sourceL]\n","            probs = []\n","            for prob, action_id in zip(probs_, action_idxs):\n","                probs.append(prob[[x for x in range(batch_size)], action_id.data])\n","        else:\n","            # return the list of len sourceL of [batch_size x sourceL]\n","            probs = probs_\n","\n","        # get the critic value fn estimates for the baseline\n","        # [batch_size]\n","        #v = self.critic_net(embedded_inputs)\n","    \n","        # [batch_size]\n","        R = self.objective_fn(actions, self.use_cuda)\n","        \n","        #return R, v, probs, actions, action_idxs\n","        return R, probs, actions, action_idxs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uTsf6pGJxSAe","colab_type":"code","colab":{}},"cell_type":"code","source":["def reward(sample_solution, USE_CUDA=False):\n","    \"\"\"\n","    Args:\n","        List of length sourceL of [batch_size] Tensors\n","    Returns:\n","        Tensor of shape [batch_size] containins rewards\n","    \"\"\"\n","    batch_size = sample_solution[0].size(0)\n","    n = len(sample_solution)\n","    tour_len = Variable(torch.zeros([batch_size]))\n","    \n","    if USE_CUDA:\n","        tour_len = tour_len.cuda()\n","\n","    for i in range(n-1):\n","        tour_len += torch.norm(sample_solution[i] - sample_solution[i+1], dim=1)\n","    \n","    tour_len += torch.norm(sample_solution[n-1] - sample_solution[0], dim=1)\n","\n","    # For TSP_20 - map to a number between 0 and 1\n","    # min_len = 3.5\n","    # max_len = 10.\n","    # TODO: generalize this for any TSP size\n","    #tour_len = -0.1538*tour_len + 1.538 \n","    #tour_len[tour_len < 0.] = 0.\n","    return tour_len"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k_VQmPNFxVxP","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"boxbNGBxxXSU","colab_type":"text"},"cell_type":"markdown","source":["# Training"]},{"metadata":{"id":"P4xJkWPxxVu6","colab_type":"code","outputId":"60028e16-f649-4635-d053-336300043b32","executionInfo":{"status":"ok","timestamp":1554073534320,"user_tz":240,"elapsed":972,"user":{"displayName":"Qiang Ma","photoUrl":"https://lh6.googleusercontent.com/-xvqSKhxJSkY/AAAAAAAAAAI/AAAAAAAAAAs/aWtprnac56k/s64/photo.jpg","userId":"05776109497485911566"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import os\n","from tqdm import tqdm \n","\n","import pprint as pp\n","import numpy as np\n","\n","import torch\n","print(torch.__version__)\n","import torch.optim as optim\n","import torch.autograd as autograd\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm_notebook"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.0.1.post2\n"],"name":"stdout"}]},{"metadata":{"id":"MYWYhTqVxVsk","colab_type":"code","colab":{}},"cell_type":"code","source":["input_dim = 2\n","embedding_dim = 128\n","hidden_dim = 128\n","max_decoding_len = 50\n","terminating_symbol = ''\n","n_glimpses = 1\n","n_process_block_iters = 3\n","tanh_exploration = 10 \n","use_tanh = True\n","beam_size = 1\n","objective_fn = reward\n","is_train = True\n","use_cuda = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NzD1ULP3xVqF","colab_type":"code","colab":{}},"cell_type":"code","source":["# build model\n","model = NeuralCombOptRL(input_dim, embedding_dim, hidden_dim,\n","                        max_decoding_len, terminating_symbol,\n","                        n_glimpses, n_process_block_iters, \n","                        tanh_exploration, use_tanh, beam_size,\n","                        objective_fn, is_train, use_cuda)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8VBssLWXxVnj","colab_type":"code","colab":{}},"cell_type":"code","source":["# the critic baseline\n","critic_exp_mvg_avg = torch.zeros(1)\n","beta = 0.8\n","\n","if use_cuda:\n","    model = model.cuda()\n","    #critic_mse = critic_mse.cuda()\n","    critic_exp_mvg_avg = critic_exp_mvg_avg.cuda()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bLv1ugjgxT4O","colab_type":"code","colab":{}},"cell_type":"code","source":["# The low learning rate doesn't work\n","actor_net_lr = 1e-3\n","critic_net_lr = 1e-3\n","actor_lr_decay_step = 5000\n","critic_lr_decay_step = 5000\n","actor_lr_decay_rate = 0.96\n","actor_lr_decay_rate = 0.96"],"execution_count":0,"outputs":[]},{"metadata":{"id":"32DqDIRLxiJ3","colab_type":"code","colab":{}},"cell_type":"code","source":["#critic_mse = torch.nn.MSELoss()\n","#critic_optim = optim.Adam(model.critic_net.parameters(), lr=float(args['critic_net_lr']))\n","actor_optim = optim.Adam(model.actor_net.parameters(), lr=actor_net_lr)\n","\n","actor_scheduler = lr_scheduler.MultiStepLR(actor_optim,\n","        range(actor_lr_decay_step, actor_lr_decay_step * 1000,actor_lr_decay_step),\n","                                           gamma=actor_lr_decay_rate)\n","\n","#critic_scheduler = lr_scheduler.MultiStepLR(critic_optim,\n","#        range(int(args['critic_lr_decay_step']), int(args['critic_lr_decay_step']) * 1000,\n","#            int(args['critic_lr_decay_step'])), gamma=float(args['critic_lr_decay_rate']))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DLfFI8aRxjgR","colab_type":"code","outputId":"448bf5a5-6f96-4c27-acaf-8c5880bda83a","executionInfo":{"status":"ok","timestamp":1554079501208,"user_tz":240,"elapsed":5957384,"user":{"displayName":"Qiang Ma","photoUrl":"https://lh6.googleusercontent.com/-xvqSKhxJSkY/AAAAAAAAAAI/AAAAAAAAAAs/aWtprnac56k/s64/photo.jpg","userId":"05776109497485911566"}},"colab":{"base_uri":"https://localhost:8080/","height":3505}},"cell_type":"code","source":["step = 0\n","log_step = 100\n","epoch = 0\n","n_epoch = 1\n","Reward = []\n","for i in range(epoch, epoch + n_epoch):\n","\n","    # put in train mode!\n","    model.train()\n","    # model.actor_net.decoder.decode_type = \"greedy\"\n","    # sample_batch is [batch_size x input_dim x sourceL]\n","    for batch_id, sample_batch in enumerate(train_dataloader):\n","\n","        bat = Variable(sample_batch)\n","        if use_cuda:\n","            bat = bat.cuda()\n","\n","        R, probs, actions, actions_idxs = model(bat)\n","    \n","        if batch_id == 0:\n","            critic_exp_mvg_avg = R.mean()\n","        else:\n","            # update critic baseline\n","            critic_exp_mvg_avg = (critic_exp_mvg_avg * beta) + ((1. - beta) * R.mean())\n","\n","        advantage = R - critic_exp_mvg_avg\n","\n","        # log probability\n","        logprobs = 0\n","        nll = 0\n","        entropy = 0\n","        # the size of probs (50 * batchsize)\n","        for prob in probs: \n","            # compute the sum of the log probability\n","            # for each tour in the batch\n","            logprob = torch.log(prob)\n","            nll += -logprob\n","            logprobs += logprob\n","       \n","        # guard against nan\n","        nll[(nll != nll).detach()] = 0.\n","        # clamp any -inf's to 0 to throw away this tour\n","        logprobs[(logprobs < -1000).detach()] = 0.\n","\n","        # multiply each time step by the advantage\n","        # advantage = Reward-Critic\n","        # reinforce = (R-b)*log(p)\n","        # R-b does not have gradient\n","        # gradient = (R-b)*d log(p)       \n","        \n","        reinforce = advantage * logprobs\n","        actor_loss = reinforce.mean()\n","\n","        actor_optim.zero_grad()\n","       \n","        actor_loss.backward()\n","\n","        max_grad_norm = 1.0\n","        # clip gradient norms\n","        # to avoid too large gradient\n","        torch.nn.utils.clip_grad_norm_(model.actor_net.parameters(),\n","                                       max_grad_norm, norm_type=2)\n","\n","        actor_optim.step()\n","        actor_scheduler.step()\n","\n","        critic_exp_mvg_avg = critic_exp_mvg_avg.detach()\n","\n","        #critic_scheduler.step()\n","\n","        #R = R.detach()\n","        #critic_loss = critic_mse(v.squeeze(1), R)\n","        #critic_optim.zero_grad()\n","        #critic_loss.backward()\n","        \n","        #torch.nn.utils.clip_grad_norm_(model.critic_net.parameters(),\n","        #        float(args['max_grad_norm']), norm_type=2)\n","\n","        #critic_optim.step()\n","        \n","        step += 1\n","\n","        if step % log_step == 0:\n","            print('epoch: {}, train_batch_id: {}, avg_reward: {}'.format(\n","                i, batch_id, R.mean().item()))\n","            example_output = []\n","            example_input = []\n","            for idx, action in enumerate(actions):\n","                example_output.append(actions_idxs[idx][0].item())\n","                example_input.append(sample_batch[0, :, idx][0])\n","            # print('Example train input: {}'.format(example_input))\n","            print('Example train output: {}'.format(example_output))\n","            Reward.append(R.mean().item())\n","\n","    # validation\n","    avg_reward = []\n","    val_step = 0\n","    model.eval()\n","    model.actor_net.decoder.decode_type = \"greedy\"\n","    \n","    for batch_id, val_batch in enumerate(tqdm_notebook(val_dataloader)):\n","        bat = Variable(val_batch)\n","    \n","        if use_cuda:\n","            bat = bat.cuda()\n","    \n","        R, probs, actions, action_idxs = model(bat)\n","        avg_reward.append(R.mean().cpu().numpy())\n","        val_step += 1.\n","    \n","    print('Validation overall avg_reward: {}'.format(np.mean(avg_reward)))\n","    print('Validation overall reward var: {}'.format(np.var(avg_reward)))\n","    \n","    # before next training loop    \n","    model.actor_net.decoder.decode_type = \"stochastic\"\n","    # generate new data\n","    training_dataset = TSPDataset(train=True, size=size,\n","                                  num_samples=train_size)\n","    train_dataloader = DataLoader(training_dataset, batch_size=batch_size,\n","                                  shuffle=True, num_workers=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch: 0, train_batch_id: 99, avg_reward: 17.68767547607422\n","Example train output: [15, 20, 30, 32, 37, 34, 23, 33, 45, 2, 7, 13, 1, 49, 40, 43, 36, 28, 26, 8, 38, 21, 39, 18, 10, 27, 4, 48, 11, 29, 47, 5, 22, 46, 3, 24, 41, 14, 35, 0, 9, 19, 17, 42, 31, 12, 16, 6, 44, 25]\n","epoch: 0, train_batch_id: 199, avg_reward: 17.257694244384766\n","Example train output: [20, 14, 43, 41, 32, 12, 38, 48, 21, 0, 44, 28, 11, 40, 24, 35, 42, 25, 22, 7, 45, 33, 49, 34, 3, 16, 4, 26, 30, 47, 37, 10, 18, 6, 8, 29, 19, 39, 31, 5, 9, 27, 13, 1, 15, 17, 23, 46, 2, 36]\n","epoch: 0, train_batch_id: 299, avg_reward: 17.48166275024414\n","Example train output: [42, 20, 11, 46, 14, 43, 13, 39, 38, 29, 44, 26, 19, 22, 17, 37, 15, 2, 16, 34, 7, 47, 8, 5, 24, 30, 12, 32, 40, 33, 21, 45, 23, 18, 10, 28, 48, 36, 9, 0, 49, 3, 4, 6, 35, 1, 31, 25, 27, 41]\n","epoch: 0, train_batch_id: 399, avg_reward: 17.0690860748291\n","Example train output: [33, 44, 17, 9, 49, 46, 47, 45, 39, 41, 38, 37, 3, 7, 20, 16, 25, 12, 14, 32, 13, 15, 21, 29, 31, 28, 19, 23, 1, 27, 18, 24, 5, 6, 26, 0, 36, 22, 8, 40, 10, 42, 30, 43, 2, 4, 48, 35, 11, 34]\n","epoch: 0, train_batch_id: 499, avg_reward: 16.97426986694336\n","Example train output: [31, 49, 21, 25, 38, 11, 41, 48, 37, 23, 27, 35, 30, 19, 15, 16, 46, 43, 26, 8, 20, 17, 29, 10, 40, 28, 0, 9, 33, 5, 1, 34, 12, 13, 39, 18, 24, 4, 22, 42, 7, 47, 44, 2, 32, 45, 14, 6, 3, 36]\n","epoch: 0, train_batch_id: 599, avg_reward: 17.267196655273438\n","Example train output: [8, 38, 7, 44, 27, 3, 45, 41, 16, 29, 15, 36, 39, 20, 49, 5, 24, 22, 18, 11, 17, 13, 46, 32, 40, 21, 6, 30, 4, 25, 28, 12, 42, 1, 31, 35, 0, 23, 19, 26, 9, 33, 2, 43, 48, 47, 10, 34, 14, 37]\n","epoch: 0, train_batch_id: 699, avg_reward: 17.793434143066406\n","Example train output: [46, 20, 11, 36, 17, 18, 43, 35, 40, 1, 44, 16, 13, 23, 33, 12, 29, 49, 30, 38, 45, 28, 21, 22, 27, 39, 10, 25, 47, 41, 19, 34, 6, 32, 7, 14, 24, 5, 0, 15, 4, 3, 2, 9, 31, 37, 8, 48, 26, 42]\n","epoch: 0, train_batch_id: 799, avg_reward: 17.213123321533203\n","Example train output: [43, 18, 1, 23, 2, 32, 34, 7, 35, 19, 14, 6, 13, 29, 16, 26, 4, 44, 41, 30, 46, 0, 5, 45, 8, 20, 25, 24, 12, 36, 39, 15, 49, 17, 37, 28, 9, 47, 3, 40, 38, 27, 22, 10, 48, 21, 33, 42, 11, 31]\n","epoch: 0, train_batch_id: 899, avg_reward: 17.36566162109375\n","Example train output: [32, 37, 20, 25, 4, 29, 48, 3, 9, 49, 18, 47, 17, 36, 14, 2, 8, 31, 21, 33, 30, 44, 22, 38, 16, 13, 41, 10, 23, 1, 35, 46, 7, 15, 39, 0, 34, 43, 5, 12, 24, 45, 26, 42, 27, 28, 6, 11, 19, 40]\n","epoch: 0, train_batch_id: 999, avg_reward: 17.21158218383789\n","Example train output: [49, 34, 14, 9, 25, 28, 48, 32, 19, 38, 17, 41, 0, 6, 40, 5, 36, 42, 27, 12, 8, 22, 13, 29, 1, 3, 44, 47, 20, 15, 33, 2, 16, 46, 11, 23, 43, 35, 37, 4, 31, 39, 18, 21, 7, 10, 30, 26, 24, 45]\n","epoch: 0, train_batch_id: 1099, avg_reward: 17.273561477661133\n","Example train output: [47, 13, 1, 27, 35, 30, 39, 12, 34, 17, 14, 36, 3, 33, 22, 2, 31, 45, 42, 44, 28, 19, 25, 4, 26, 18, 5, 6, 15, 43, 23, 9, 32, 16, 21, 0, 40, 24, 37, 41, 49, 48, 7, 10, 8, 11, 38, 20, 29, 46]\n","epoch: 0, train_batch_id: 1199, avg_reward: 17.135452270507812\n","Example train output: [0, 14, 35, 13, 2, 1, 23, 19, 10, 27, 46, 8, 26, 38, 9, 24, 48, 20, 4, 12, 3, 34, 42, 15, 40, 39, 30, 6, 45, 31, 17, 28, 44, 32, 36, 29, 18, 47, 25, 43, 16, 11, 49, 33, 37, 5, 7, 22, 41, 21]\n","epoch: 0, train_batch_id: 1299, avg_reward: 16.924787521362305\n","Example train output: [30, 22, 32, 31, 3, 24, 10, 34, 17, 5, 33, 7, 12, 27, 6, 40, 11, 36, 8, 29, 19, 23, 1, 46, 15, 13, 26, 39, 2, 16, 49, 41, 28, 47, 21, 4, 25, 48, 9, 38, 43, 14, 20, 42, 18, 35, 0, 44, 45, 37]\n","epoch: 0, train_batch_id: 1399, avg_reward: 17.108524322509766\n","Example train output: [18, 43, 37, 31, 47, 28, 41, 20, 33, 19, 42, 44, 34, 17, 8, 1, 29, 15, 40, 48, 13, 49, 14, 9, 6, 39, 11, 22, 12, 10, 45, 36, 26, 30, 16, 21, 5, 3, 4, 24, 2, 46, 32, 0, 35, 25, 23, 38, 7, 27]\n","epoch: 0, train_batch_id: 1499, avg_reward: 17.303258895874023\n","Example train output: [10, 32, 16, 44, 3, 30, 11, 39, 6, 47, 31, 18, 19, 25, 4, 40, 35, 28, 26, 34, 48, 27, 17, 24, 42, 7, 38, 43, 45, 21, 49, 13, 9, 1, 22, 15, 5, 33, 14, 23, 0, 12, 20, 8, 37, 2, 41, 46, 36, 29]\n","epoch: 0, train_batch_id: 1599, avg_reward: 17.257083892822266\n","Example train output: [8, 38, 9, 25, 27, 6, 40, 15, 36, 22, 24, 3, 49, 10, 46, 20, 42, 2, 30, 37, 18, 5, 28, 41, 21, 7, 34, 47, 43, 29, 12, 17, 23, 16, 48, 13, 31, 0, 14, 33, 35, 11, 19, 26, 45, 44, 39, 1, 4, 32]\n","epoch: 0, train_batch_id: 1699, avg_reward: 16.959808349609375\n","Example train output: [38, 34, 15, 9, 4, 24, 0, 43, 13, 25, 42, 44, 48, 35, 7, 46, 26, 33, 22, 6, 10, 49, 28, 20, 12, 29, 27, 32, 14, 39, 45, 3, 18, 19, 8, 36, 21, 30, 31, 41, 47, 2, 1, 16, 23, 5, 11, 17, 40, 37]\n","epoch: 0, train_batch_id: 1799, avg_reward: 17.068336486816406\n","Example train output: [29, 47, 41, 45, 32, 14, 20, 4, 26, 23, 44, 7, 34, 31, 3, 22, 27, 16, 39, 30, 49, 15, 43, 35, 18, 33, 46, 17, 37, 21, 38, 0, 48, 12, 13, 5, 36, 1, 6, 11, 2, 19, 10, 8, 40, 28, 42, 25, 9, 24]\n","epoch: 0, train_batch_id: 1899, avg_reward: 17.474180221557617\n","Example train output: [39, 0, 29, 30, 47, 36, 25, 1, 2, 24, 44, 16, 5, 23, 40, 3, 19, 31, 49, 26, 28, 20, 4, 6, 27, 15, 9, 10, 48, 37, 8, 33, 46, 17, 7, 11, 14, 18, 12, 38, 21, 41, 42, 13, 35, 34, 43, 22, 32, 45]\n","epoch: 0, train_batch_id: 1999, avg_reward: 18.319530487060547\n","Example train output: [21, 24, 36, 1, 46, 32, 49, 8, 40, 17, 43, 4, 42, 31, 26, 28, 38, 44, 9, 16, 30, 47, 19, 6, 33, 15, 22, 29, 34, 7, 41, 5, 37, 35, 25, 3, 10, 48, 11, 12, 27, 45, 2, 0, 14, 18, 13, 23, 39, 20]\n","epoch: 0, train_batch_id: 2099, avg_reward: 16.942581176757812\n","Example train output: [20, 47, 25, 19, 24, 1, 16, 26, 21, 48, 7, 30, 4, 9, 2, 8, 33, 46, 14, 18, 0, 42, 23, 11, 28, 45, 29, 6, 34, 13, 41, 5, 10, 12, 40, 49, 31, 32, 3, 17, 37, 36, 22, 43, 44, 35, 15, 39, 38, 27]\n","epoch: 0, train_batch_id: 2199, avg_reward: 17.265579223632812\n","Example train output: [14, 49, 10, 0, 38, 8, 9, 39, 11, 3, 28, 41, 27, 33, 36, 5, 24, 23, 6, 37, 45, 20, 31, 4, 7, 15, 48, 34, 22, 16, 1, 29, 21, 25, 35, 12, 30, 42, 13, 17, 32, 2, 47, 40, 18, 26, 46, 19, 43, 44]\n","epoch: 0, train_batch_id: 2299, avg_reward: 17.329673767089844\n","Example train output: [36, 34, 16, 14, 38, 30, 43, 4, 2, 6, 49, 24, 7, 18, 26, 40, 32, 27, 10, 47, 9, 19, 23, 11, 5, 12, 37, 45, 20, 8, 39, 1, 48, 22, 28, 29, 0, 31, 33, 13, 44, 41, 15, 42, 3, 21, 46, 25, 35, 17]\n","epoch: 0, train_batch_id: 2399, avg_reward: 17.97271728515625\n","Example train output: [15, 7, 27, 5, 1, 26, 46, 44, 2, 6, 49, 43, 45, 16, 18, 12, 37, 11, 36, 41, 25, 20, 31, 4, 24, 29, 0, 3, 17, 14, 34, 13, 22, 33, 30, 32, 8, 48, 9, 19, 47, 21, 35, 42, 28, 40, 39, 10, 38, 23]\n","epoch: 0, train_batch_id: 2499, avg_reward: 16.934507369995117\n","Example train output: [14, 19, 12, 28, 11, 6, 9, 10, 7, 41, 37, 22, 18, 29, 32, 5, 35, 4, 23, 44, 17, 45, 3, 26, 16, 1, 39, 34, 15, 48, 31, 38, 49, 42, 21, 46, 40, 0, 47, 20, 33, 36, 13, 2, 24, 43, 8, 27, 25, 30]\n","epoch: 0, train_batch_id: 2599, avg_reward: 16.64907455444336\n","Example train output: [48, 18, 43, 22, 30, 16, 3, 32, 23, 1, 0, 47, 14, 49, 27, 34, 11, 10, 17, 21, 2, 42, 20, 45, 13, 35, 6, 37, 8, 19, 24, 39, 36, 28, 15, 29, 41, 38, 26, 5, 9, 33, 7, 46, 12, 40, 44, 4, 25, 31]\n","epoch: 0, train_batch_id: 2699, avg_reward: 16.849361419677734\n","Example train output: [21, 27, 4, 38, 17, 6, 14, 2, 32, 39, 11, 28, 19, 12, 5, 31, 10, 7, 42, 9, 16, 49, 15, 41, 44, 47, 20, 26, 0, 36, 37, 13, 46, 34, 18, 35, 48, 1, 24, 33, 25, 29, 8, 30, 40, 23, 22, 43, 3, 45]\n","epoch: 0, train_batch_id: 2799, avg_reward: 18.169694900512695\n","Example train output: [29, 0, 41, 26, 22, 19, 40, 44, 33, 20, 46, 48, 39, 38, 1, 18, 6, 15, 42, 7, 14, 49, 16, 28, 5, 11, 32, 13, 25, 17, 47, 30, 23, 37, 45, 8, 21, 34, 12, 31, 36, 2, 43, 10, 27, 24, 35, 4, 9, 3]\n","epoch: 0, train_batch_id: 2899, avg_reward: 17.602397918701172\n","Example train output: [46, 44, 24, 29, 22, 9, 13, 17, 34, 8, 43, 25, 5, 42, 40, 12, 30, 36, 37, 49, 41, 32, 10, 18, 35, 2, 4, 28, 3, 38, 27, 48, 6, 21, 0, 14, 31, 33, 39, 47, 15, 1, 23, 20, 26, 11, 45, 16, 7, 19]\n","epoch: 0, train_batch_id: 2999, avg_reward: 17.306486129760742\n","Example train output: [8, 34, 3, 38, 39, 20, 2, 43, 4, 48, 31, 42, 1, 5, 30, 46, 15, 22, 47, 33, 32, 29, 26, 23, 6, 27, 44, 37, 28, 49, 9, 25, 45, 16, 40, 13, 18, 17, 0, 36, 12, 35, 41, 24, 11, 10, 19, 14, 7, 21]\n","epoch: 0, train_batch_id: 3099, avg_reward: 17.244874954223633\n","Example train output: [28, 25, 38, 8, 14, 42, 16, 21, 23, 37, 36, 11, 3, 20, 19, 47, 34, 49, 6, 17, 24, 44, 46, 35, 40, 29, 5, 39, 1, 45, 22, 33, 48, 43, 9, 4, 0, 41, 30, 32, 15, 27, 12, 18, 26, 13, 7, 2, 31, 10]\n","epoch: 0, train_batch_id: 3199, avg_reward: 16.562347412109375\n","Example train output: [33, 16, 1, 19, 11, 3, 20, 34, 17, 28, 26, 2, 30, 23, 5, 39, 32, 24, 22, 41, 9, 48, 13, 37, 12, 6, 31, 21, 42, 25, 36, 35, 49, 8, 43, 38, 29, 40, 7, 15, 44, 27, 10, 0, 14, 47, 18, 4, 46, 45]\n","epoch: 0, train_batch_id: 3299, avg_reward: 16.905227661132812\n","Example train output: [28, 18, 36, 0, 1, 31, 32, 23, 17, 38, 3, 27, 44, 19, 29, 20, 7, 9, 2, 15, 5, 6, 33, 16, 25, 47, 30, 21, 46, 41, 37, 8, 35, 12, 39, 48, 34, 22, 13, 14, 42, 45, 49, 10, 4, 40, 24, 26, 43, 11]\n","epoch: 0, train_batch_id: 3399, avg_reward: 17.229862213134766\n","Example train output: [28, 27, 37, 36, 19, 26, 21, 41, 48, 33, 40, 24, 15, 46, 4, 10, 39, 25, 17, 0, 38, 49, 14, 22, 34, 7, 20, 44, 5, 23, 42, 1, 30, 3, 9, 16, 45, 12, 11, 31, 29, 2, 35, 6, 13, 18, 43, 32, 47, 8]\n","epoch: 0, train_batch_id: 3499, avg_reward: 16.743146896362305\n","Example train output: [3, 35, 24, 1, 41, 31, 17, 18, 6, 45, 16, 13, 9, 49, 4, 25, 0, 7, 8, 23, 29, 20, 34, 43, 46, 39, 28, 32, 5, 10, 37, 38, 36, 47, 30, 12, 11, 15, 48, 22, 26, 21, 40, 42, 2, 33, 27, 14, 44, 19]\n","epoch: 0, train_batch_id: 3599, avg_reward: 16.75096893310547\n","Example train output: [2, 21, 42, 3, 18, 0, 49, 32, 35, 46, 48, 37, 41, 10, 33, 44, 8, 20, 13, 9, 45, 28, 16, 15, 23, 6, 39, 27, 29, 40, 38, 14, 11, 25, 31, 22, 43, 36, 34, 4, 26, 17, 24, 30, 12, 1, 5, 7, 47, 19]\n","epoch: 0, train_batch_id: 3699, avg_reward: 17.437156677246094\n","Example train output: [24, 35, 18, 6, 28, 14, 40, 37, 49, 31, 3, 26, 47, 45, 4, 46, 1, 7, 10, 17, 44, 34, 25, 13, 16, 48, 32, 9, 19, 29, 30, 21, 41, 0, 33, 39, 5, 15, 12, 11, 22, 43, 38, 2, 36, 23, 27, 8, 42, 20]\n","epoch: 0, train_batch_id: 3799, avg_reward: 16.777637481689453\n","Example train output: [25, 42, 9, 30, 4, 3, 32, 38, 39, 12, 23, 49, 16, 20, 43, 35, 44, 19, 0, 14, 15, 22, 37, 28, 34, 11, 10, 8, 33, 24, 5, 27, 17, 2, 46, 31, 36, 41, 6, 48, 18, 26, 13, 21, 29, 1, 7, 45, 40, 47]\n","epoch: 0, train_batch_id: 3899, avg_reward: 16.97039031982422\n","Example train output: [32, 48, 30, 39, 19, 29, 10, 14, 11, 43, 27, 23, 21, 2, 34, 18, 40, 22, 6, 3, 41, 36, 4, 42, 28, 47, 8, 31, 49, 26, 24, 0, 20, 38, 25, 35, 17, 15, 46, 5, 16, 1, 13, 7, 33, 45, 12, 9, 37, 44]\n","epoch: 0, train_batch_id: 3999, avg_reward: 16.9527587890625\n","Example train output: [49, 19, 14, 39, 24, 15, 9, 0, 11, 36, 17, 40, 45, 43, 1, 16, 27, 6, 7, 33, 37, 13, 4, 23, 2, 35, 20, 44, 12, 29, 8, 32, 5, 10, 38, 47, 26, 46, 41, 30, 48, 28, 34, 3, 18, 21, 42, 31, 25, 22]\n","epoch: 0, train_batch_id: 4099, avg_reward: 16.580066680908203\n","Example train output: [6, 21, 23, 24, 4, 30, 22, 20, 16, 31, 9, 0, 14, 10, 25, 37, 42, 17, 40, 26, 27, 33, 1, 19, 12, 46, 36, 29, 5, 39, 13, 34, 32, 49, 11, 8, 7, 3, 18, 44, 28, 15, 41, 38, 47, 48, 2, 35, 45, 43]\n","epoch: 0, train_batch_id: 4199, avg_reward: 16.807767868041992\n","Example train output: [27, 28, 47, 48, 35, 23, 3, 1, 33, 38, 36, 41, 17, 11, 31, 8, 9, 32, 49, 26, 37, 34, 43, 13, 29, 10, 39, 42, 5, 4, 6, 25, 30, 2, 40, 44, 19, 14, 0, 22, 15, 24, 46, 16, 45, 12, 20, 18, 21, 7]\n","epoch: 0, train_batch_id: 4299, avg_reward: 16.7429256439209\n","Example train output: [28, 16, 13, 7, 46, 32, 38, 12, 39, 42, 31, 35, 43, 34, 9, 40, 21, 23, 27, 4, 22, 17, 37, 2, 26, 14, 1, 44, 24, 36, 49, 41, 20, 18, 33, 30, 19, 48, 0, 29, 47, 15, 8, 45, 25, 11, 6, 10, 3, 5]\n","epoch: 0, train_batch_id: 4399, avg_reward: 16.89980697631836\n","Example train output: [47, 35, 37, 48, 27, 34, 30, 26, 44, 2, 8, 43, 49, 46, 6, 38, 25, 13, 19, 24, 5, 16, 17, 40, 15, 21, 32, 10, 29, 12, 7, 18, 42, 3, 23, 0, 4, 22, 11, 31, 9, 14, 36, 39, 28, 33, 41, 45, 20, 1]\n","epoch: 0, train_batch_id: 4499, avg_reward: 16.86855697631836\n","Example train output: [22, 23, 48, 7, 43, 17, 10, 27, 19, 24, 29, 20, 3, 14, 15, 1, 40, 44, 21, 5, 38, 46, 49, 0, 11, 26, 33, 42, 34, 41, 37, 47, 35, 2, 4, 13, 31, 45, 6, 12, 8, 25, 30, 18, 9, 32, 36, 28, 39, 16]\n","epoch: 0, train_batch_id: 4599, avg_reward: 16.626972198486328\n","Example train output: [6, 8, 37, 47, 23, 36, 0, 31, 49, 25, 48, 24, 34, 5, 13, 45, 20, 12, 40, 4, 10, 30, 39, 18, 9, 44, 42, 33, 22, 35, 28, 21, 16, 46, 27, 7, 38, 3, 14, 19, 43, 1, 2, 41, 11, 32, 17, 29, 15, 26]\n","epoch: 0, train_batch_id: 4699, avg_reward: 16.84837532043457\n","Example train output: [2, 43, 35, 12, 14, 0, 20, 38, 11, 8, 31, 39, 4, 30, 26, 41, 1, 23, 27, 37, 13, 44, 40, 3, 36, 5, 34, 17, 15, 47, 24, 7, 22, 45, 33, 9, 10, 28, 29, 16, 19, 49, 48, 32, 46, 25, 42, 6, 21, 18]\n","epoch: 0, train_batch_id: 4799, avg_reward: 16.959041595458984\n","Example train output: [6, 11, 23, 9, 24, 22, 14, 26, 25, 45, 15, 28, 31, 1, 36, 0, 16, 4, 2, 41, 8, 7, 10, 21, 49, 38, 39, 47, 20, 34, 43, 44, 46, 29, 17, 32, 12, 18, 40, 3, 5, 33, 27, 19, 30, 48, 35, 42, 37, 13]\n","epoch: 0, train_batch_id: 4899, avg_reward: 17.138431549072266\n","Example train output: [36, 1, 45, 18, 41, 39, 7, 19, 38, 17, 0, 44, 13, 32, 29, 20, 35, 21, 24, 12, 48, 42, 10, 33, 27, 34, 25, 37, 5, 2, 49, 23, 43, 47, 30, 9, 46, 16, 40, 26, 4, 15, 8, 11, 14, 28, 22, 6, 31, 3]\n","epoch: 0, train_batch_id: 4999, avg_reward: 17.029644012451172\n","Example train output: [6, 49, 3, 0, 22, 16, 43, 5, 45, 15, 29, 12, 34, 11, 19, 39, 46, 28, 20, 26, 35, 41, 38, 31, 48, 24, 8, 9, 33, 25, 23, 30, 7, 10, 27, 2, 47, 37, 14, 44, 17, 4, 32, 13, 36, 40, 1, 18, 42, 21]\n","epoch: 0, train_batch_id: 5099, avg_reward: 17.085834503173828\n","Example train output: [37, 7, 33, 39, 0, 10, 45, 41, 44, 35, 28, 13, 14, 20, 17, 6, 8, 1, 29, 46, 26, 2, 34, 43, 30, 49, 18, 3, 32, 19, 48, 22, 38, 47, 21, 24, 12, 23, 42, 36, 16, 9, 4, 40, 25, 5, 31, 15, 11, 27]\n","epoch: 0, train_batch_id: 5199, avg_reward: 16.761516571044922\n","Example train output: [42, 9, 10, 4, 28, 30, 11, 48, 29, 47, 27, 37, 3, 40, 17, 2, 21, 8, 13, 38, 18, 19, 15, 46, 16, 43, 23, 12, 1, 49, 31, 34, 24, 32, 22, 6, 33, 7, 25, 20, 5, 39, 41, 36, 44, 35, 14, 0, 26, 45]\n","epoch: 0, train_batch_id: 5299, avg_reward: 16.657907485961914\n","Example train output: [25, 34, 8, 39, 5, 47, 14, 20, 40, 43, 23, 41, 18, 22, 10, 31, 45, 29, 46, 12, 21, 0, 36, 35, 9, 4, 37, 7, 3, 42, 49, 17, 6, 27, 15, 48, 19, 38, 16, 30, 24, 13, 33, 28, 26, 32, 2, 11, 44, 1]\n","epoch: 0, train_batch_id: 5399, avg_reward: 16.97037124633789\n","Example train output: [47, 39, 26, 36, 0, 17, 34, 6, 44, 1, 29, 28, 22, 10, 33, 16, 3, 21, 11, 41, 14, 38, 20, 25, 23, 37, 13, 18, 24, 2, 8, 9, 31, 46, 35, 43, 7, 40, 30, 12, 42, 32, 45, 4, 48, 27, 5, 19, 15, 49]\n","epoch: 0, train_batch_id: 5499, avg_reward: 17.729955673217773\n","Example train output: [31, 47, 45, 44, 5, 10, 19, 35, 16, 22, 36, 25, 4, 27, 30, 2, 1, 37, 15, 41, 49, 28, 12, 42, 40, 33, 32, 38, 43, 48, 23, 0, 29, 21, 9, 7, 11, 20, 24, 3, 39, 14, 8, 17, 46, 13, 18, 34, 6, 26]\n","epoch: 0, train_batch_id: 5599, avg_reward: 16.60460090637207\n","Example train output: [25, 6, 40, 34, 28, 30, 3, 9, 24, 14, 36, 41, 4, 19, 16, 42, 0, 43, 1, 7, 29, 37, 44, 47, 17, 45, 21, 48, 22, 38, 20, 18, 13, 33, 27, 11, 10, 31, 8, 49, 39, 23, 32, 5, 46, 35, 15, 26, 12, 2]\n","epoch: 0, train_batch_id: 5699, avg_reward: 16.78877830505371\n","Example train output: [12, 19, 32, 13, 16, 22, 2, 6, 39, 27, 29, 10, 48, 47, 46, 38, 14, 7, 33, 31, 36, 25, 9, 0, 21, 44, 4, 45, 37, 5, 11, 30, 43, 40, 34, 18, 20, 8, 17, 49, 23, 41, 42, 28, 26, 3, 35, 1, 24, 15]\n","epoch: 0, train_batch_id: 5799, avg_reward: 16.640872955322266\n","Example train output: [0, 20, 1, 11, 6, 42, 2, 47, 38, 43, 16, 19, 23, 32, 22, 28, 30, 17, 26, 44, 33, 12, 39, 7, 8, 34, 40, 27, 10, 36, 21, 31, 18, 35, 49, 3, 9, 4, 29, 41, 37, 48, 25, 46, 5, 13, 45, 14, 15, 24]\n","epoch: 0, train_batch_id: 5899, avg_reward: 16.577537536621094\n","Example train output: [39, 41, 31, 35, 2, 20, 6, 49, 33, 21, 22, 3, 19, 32, 28, 26, 30, 27, 34, 10, 5, 14, 18, 24, 9, 42, 23, 7, 11, 47, 16, 38, 17, 4, 36, 13, 37, 29, 40, 0, 43, 12, 15, 44, 8, 45, 48, 46, 1, 25]\n","epoch: 0, train_batch_id: 5999, avg_reward: 16.936908721923828\n","Example train output: [42, 47, 29, 39, 46, 13, 22, 2, 18, 5, 36, 31, 30, 21, 15, 17, 45, 9, 40, 35, 41, 7, 8, 25, 33, 11, 34, 23, 44, 37, 6, 27, 32, 10, 16, 12, 38, 0, 24, 48, 28, 14, 49, 26, 1, 3, 20, 43, 4, 19]\n","epoch: 0, train_batch_id: 6099, avg_reward: 16.68242645263672\n","Example train output: [4, 33, 16, 42, 12, 38, 21, 6, 2, 7, 5, 19, 1, 46, 23, 43, 22, 32, 27, 39, 29, 40, 44, 18, 35, 14, 10, 30, 0, 37, 25, 48, 8, 24, 26, 28, 45, 20, 36, 47, 49, 13, 17, 3, 9, 41, 34, 15, 31, 11]\n","epoch: 0, train_batch_id: 6199, avg_reward: 17.255672454833984\n","Example train output: [16, 30, 28, 26, 27, 7, 21, 19, 32, 45, 38, 42, 1, 13, 46, 37, 5, 43, 36, 0, 6, 11, 35, 41, 2, 14, 22, 23, 44, 8, 3, 17, 10, 31, 25, 9, 20, 48, 29, 39, 12, 24, 15, 33, 18, 4, 34, 47, 40, 49]\n","epoch: 0, train_batch_id: 6299, avg_reward: 16.934633255004883\n","Example train output: [44, 49, 29, 45, 0, 1, 38, 41, 13, 28, 39, 35, 34, 26, 21, 16, 23, 6, 10, 37, 24, 2, 4, 5, 42, 43, 9, 36, 15, 31, 18, 25, 33, 40, 12, 32, 20, 14, 19, 3, 46, 11, 48, 27, 22, 8, 30, 7, 47, 17]\n","epoch: 0, train_batch_id: 6399, avg_reward: 16.437273025512695\n","Example train output: [22, 49, 36, 43, 28, 38, 47, 24, 34, 44, 11, 15, 26, 13, 46, 39, 8, 18, 40, 29, 17, 35, 25, 4, 48, 19, 37, 30, 3, 45, 32, 20, 2, 31, 27, 16, 1, 21, 6, 12, 0, 5, 10, 7, 14, 23, 41, 9, 33, 42]\n","epoch: 0, train_batch_id: 6499, avg_reward: 17.040401458740234\n","Example train output: [28, 33, 17, 38, 12, 16, 7, 5, 13, 45, 39, 23, 3, 9, 34, 4, 36, 1, 48, 18, 47, 46, 19, 14, 10, 24, 35, 32, 22, 11, 27, 30, 31, 0, 15, 41, 8, 44, 2, 20, 37, 40, 6, 21, 26, 43, 42, 25, 29, 49]\n","epoch: 0, train_batch_id: 6599, avg_reward: 12.356466293334961\n","Example train output: [18, 20, 42, 15, 28, 43, 3, 31, 11, 38, 35, 24, 14, 40, 5, 49, 30, 9, 10, 33, 8, 4, 16, 37, 26, 45, 19, 12, 22, 34, 41, 17, 29, 0, 48, 44, 23, 27, 46, 21, 2, 6, 1, 36, 25, 32, 7, 39, 13, 47]\n","epoch: 0, train_batch_id: 6699, avg_reward: 10.586816787719727\n","Example train output: [22, 28, 35, 21, 30, 12, 0, 19, 47, 16, 46, 41, 37, 14, 3, 44, 31, 43, 45, 48, 20, 36, 27, 42, 4, 7, 24, 26, 23, 34, 33, 38, 40, 10, 11, 32, 13, 49, 25, 29, 5, 39, 8, 6, 15, 1, 9, 18, 2, 17]\n","epoch: 0, train_batch_id: 6799, avg_reward: 9.73804759979248\n","Example train output: [42, 14, 17, 0, 7, 9, 21, 47, 34, 26, 38, 30, 4, 16, 12, 41, 36, 23, 49, 18, 37, 20, 5, 40, 43, 8, 10, 3, 1, 13, 28, 44, 6, 31, 2, 33, 29, 11, 45, 35, 15, 24, 39, 48, 19, 25, 32, 22, 46, 27]\n","epoch: 0, train_batch_id: 6899, avg_reward: 9.136409759521484\n","Example train output: [26, 11, 40, 18, 46, 3, 41, 16, 34, 5, 17, 31, 24, 30, 6, 21, 19, 27, 7, 37, 39, 2, 22, 10, 8, 35, 44, 47, 15, 38, 14, 1, 33, 49, 45, 28, 9, 0, 25, 12, 20, 48, 23, 29, 4, 13, 32, 36, 43, 42]\n","epoch: 0, train_batch_id: 6999, avg_reward: 8.926204681396484\n","Example train output: [42, 49, 7, 20, 25, 32, 2, 47, 11, 10, 43, 15, 29, 16, 39, 9, 3, 5, 34, 18, 23, 4, 12, 24, 33, 19, 1, 17, 8, 38, 46, 13, 36, 26, 31, 35, 27, 45, 0, 14, 22, 30, 44, 21, 6, 40, 28, 37, 48, 41]\n","epoch: 0, train_batch_id: 7099, avg_reward: 8.45364761352539\n","Example train output: [26, 42, 19, 30, 29, 43, 40, 27, 46, 12, 24, 1, 28, 47, 20, 16, 17, 13, 2, 0, 45, 10, 33, 4, 21, 22, 3, 38, 11, 48, 41, 36, 25, 15, 44, 9, 23, 5, 6, 14, 35, 37, 18, 32, 8, 31, 49, 7, 39, 34]\n","epoch: 0, train_batch_id: 7199, avg_reward: 8.215137481689453\n","Example train output: [14, 24, 49, 40, 45, 38, 31, 20, 13, 17, 29, 21, 3, 36, 30, 9, 22, 1, 32, 15, 43, 2, 37, 18, 23, 27, 41, 5, 7, 8, 39, 4, 33, 16, 26, 34, 12, 28, 35, 44, 10, 11, 6, 25, 47, 46, 0, 42, 19, 48]\n","epoch: 0, train_batch_id: 7299, avg_reward: 7.934445381164551\n","Example train output: [48, 45, 11, 30, 1, 12, 46, 4, 33, 23, 9, 24, 7, 49, 37, 36, 3, 5, 0, 32, 42, 39, 25, 43, 29, 6, 20, 21, 26, 15, 8, 13, 34, 44, 31, 38, 16, 10, 19, 17, 41, 18, 35, 27, 40, 2, 47, 22, 14, 28]\n","epoch: 0, train_batch_id: 7399, avg_reward: 7.791155815124512\n","Example train output: [45, 21, 39, 6, 47, 13, 1, 41, 8, 15, 48, 10, 23, 22, 36, 9, 49, 11, 31, 3, 7, 28, 17, 32, 27, 2, 46, 44, 37, 12, 29, 35, 42, 19, 34, 30, 16, 24, 0, 25, 40, 26, 20, 14, 43, 5, 18, 4, 38, 33]\n","epoch: 0, train_batch_id: 7499, avg_reward: 7.767943859100342\n","Example train output: [31, 3, 23, 32, 14, 20, 26, 11, 18, 46, 44, 22, 8, 29, 2, 39, 17, 0, 19, 37, 28, 45, 27, 40, 34, 21, 10, 43, 4, 33, 12, 16, 42, 24, 25, 49, 35, 41, 13, 38, 47, 36, 5, 6, 48, 7, 15, 30, 9, 1]\n","epoch: 0, train_batch_id: 7599, avg_reward: 7.5126824378967285\n","Example train output: [47, 26, 15, 18, 17, 35, 34, 5, 12, 11, 16, 14, 13, 41, 6, 29, 8, 45, 38, 39, 43, 21, 42, 32, 31, 40, 25, 37, 4, 20, 0, 36, 48, 22, 2, 46, 3, 44, 28, 24, 27, 30, 19, 49, 1, 9, 23, 10, 7, 33]\n","epoch: 0, train_batch_id: 7699, avg_reward: 7.330972671508789\n","Example train output: [3, 40, 6, 42, 26, 1, 45, 25, 18, 32, 13, 35, 33, 29, 20, 44, 49, 37, 24, 15, 48, 23, 30, 12, 4, 17, 8, 27, 47, 41, 22, 28, 38, 10, 14, 31, 43, 5, 34, 9, 21, 2, 39, 11, 46, 36, 7, 16, 19, 0]\n","epoch: 0, train_batch_id: 7799, avg_reward: 7.54127311706543\n","Example train output: [34, 29, 12, 20, 32, 43, 27, 26, 9, 45, 41, 36, 3, 14, 31, 4, 37, 35, 40, 10, 33, 44, 1, 7, 46, 22, 48, 39, 13, 18, 24, 47, 0, 2, 28, 30, 49, 11, 16, 42, 38, 23, 21, 5, 25, 6, 19, 8, 17, 15]\n","epoch: 0, train_batch_id: 7899, avg_reward: 7.393662452697754\n","Example train output: [31, 3, 16, 38, 11, 18, 9, 39, 32, 43, 26, 36, 25, 44, 35, 41, 46, 15, 33, 47, 27, 1, 48, 12, 8, 22, 7, 30, 40, 37, 45, 0, 28, 23, 29, 2, 21, 13, 24, 34, 5, 14, 4, 42, 6, 49, 17, 20, 19, 10]\n","epoch: 0, train_batch_id: 7999, avg_reward: 7.208779335021973\n","Example train output: [22, 42, 31, 48, 44, 26, 13, 2, 16, 4, 27, 15, 36, 18, 12, 29, 49, 33, 19, 0, 10, 14, 43, 24, 41, 8, 20, 9, 34, 28, 3, 45, 47, 7, 35, 17, 23, 25, 37, 11, 1, 5, 30, 21, 6, 38, 32, 46, 40, 39]\n","epoch: 0, train_batch_id: 8099, avg_reward: 7.679110050201416\n","Example train output: [39, 13, 2, 43, 28, 32, 18, 23, 27, 21, 49, 37, 15, 16, 5, 29, 6, 0, 45, 35, 11, 44, 20, 8, 22, 47, 34, 25, 9, 26, 30, 31, 7, 41, 10, 14, 33, 19, 3, 36, 38, 1, 24, 12, 40, 4, 17, 42, 46, 48]\n","epoch: 0, train_batch_id: 8199, avg_reward: 7.246831893920898\n","Example train output: [46, 19, 28, 14, 6, 41, 2, 20, 33, 25, 42, 34, 18, 29, 26, 23, 47, 35, 37, 17, 9, 13, 31, 43, 16, 12, 5, 24, 22, 4, 40, 10, 49, 39, 3, 8, 48, 38, 27, 7, 32, 15, 21, 30, 0, 36, 11, 45, 44, 1]\n","epoch: 0, train_batch_id: 8299, avg_reward: 7.119370460510254\n","Example train output: [36, 34, 2, 28, 15, 13, 17, 7, 42, 21, 33, 14, 26, 5, 9, 24, 46, 12, 32, 29, 18, 0, 30, 20, 43, 37, 35, 8, 41, 19, 22, 38, 6, 11, 48, 4, 31, 40, 39, 47, 49, 1, 3, 25, 44, 27, 16, 10, 23, 45]\n","epoch: 0, train_batch_id: 8399, avg_reward: 7.252811431884766\n","Example train output: [39, 17, 5, 37, 42, 20, 18, 44, 11, 47, 34, 8, 35, 32, 10, 3, 45, 31, 7, 24, 2, 19, 9, 38, 43, 1, 49, 22, 16, 12, 21, 0, 46, 25, 13, 29, 28, 33, 30, 41, 15, 27, 23, 26, 4, 6, 40, 48, 36, 14]\n","epoch: 0, train_batch_id: 8499, avg_reward: 7.244790077209473\n","Example train output: [9, 23, 18, 42, 34, 3, 17, 14, 15, 1, 27, 41, 35, 45, 5, 44, 32, 31, 8, 48, 29, 49, 0, 47, 26, 19, 40, 16, 13, 38, 7, 37, 12, 24, 30, 11, 46, 43, 25, 22, 28, 20, 33, 21, 6, 39, 36, 10, 2, 4]\n","epoch: 0, train_batch_id: 8599, avg_reward: 7.3148908615112305\n","Example train output: [42, 9, 48, 33, 49, 37, 36, 26, 2, 43, 7, 20, 32, 29, 16, 3, 22, 45, 6, 28, 24, 47, 46, 39, 40, 38, 0, 35, 27, 13, 34, 17, 44, 15, 21, 4, 8, 11, 41, 10, 18, 30, 5, 23, 1, 14, 12, 31, 19, 25]\n","epoch: 0, train_batch_id: 8699, avg_reward: 7.144269943237305\n","Example train output: [17, 8, 49, 13, 31, 25, 11, 1, 36, 42, 0, 30, 27, 46, 16, 3, 26, 44, 39, 40, 21, 23, 28, 41, 33, 35, 43, 10, 38, 7, 29, 6, 37, 19, 22, 14, 20, 4, 45, 15, 12, 47, 2, 18, 24, 9, 32, 5, 34, 48]\n","epoch: 0, train_batch_id: 8799, avg_reward: 7.22873067855835\n","Example train output: [29, 26, 48, 42, 1, 27, 36, 40, 14, 24, 34, 4, 15, 32, 20, 10, 6, 3, 37, 0, 16, 43, 22, 33, 28, 13, 7, 5, 45, 21, 12, 25, 47, 41, 31, 30, 2, 9, 23, 19, 35, 18, 44, 49, 39, 17, 46, 38, 8, 11]\n","epoch: 0, train_batch_id: 8899, avg_reward: 7.157253265380859\n","Example train output: [30, 17, 38, 5, 27, 16, 21, 0, 22, 25, 20, 7, 8, 33, 13, 12, 40, 45, 2, 19, 18, 6, 26, 42, 46, 31, 24, 1, 43, 3, 10, 47, 23, 15, 29, 48, 4, 44, 14, 35, 39, 41, 11, 49, 32, 36, 9, 34, 28, 37]\n","epoch: 0, train_batch_id: 8999, avg_reward: 7.119638442993164\n","Example train output: [45, 43, 8, 3, 0, 46, 42, 2, 9, 21, 37, 7, 25, 19, 17, 36, 32, 48, 38, 35, 27, 14, 33, 28, 47, 1, 22, 10, 41, 30, 6, 4, 31, 5, 26, 11, 15, 44, 23, 24, 39, 29, 49, 16, 12, 13, 20, 40, 18, 34]\n","epoch: 0, train_batch_id: 9099, avg_reward: 7.086285591125488\n","Example train output: [4, 39, 34, 21, 12, 8, 26, 1, 40, 29, 18, 48, 31, 33, 22, 35, 42, 37, 45, 0, 13, 9, 20, 25, 6, 5, 38, 7, 3, 15, 27, 16, 32, 23, 41, 14, 10, 24, 2, 19, 46, 30, 28, 36, 43, 44, 11, 47, 17, 49]\n","epoch: 0, train_batch_id: 9199, avg_reward: 7.019648551940918\n","Example train output: [9, 30, 24, 16, 17, 4, 40, 38, 37, 19, 44, 28, 43, 36, 7, 47, 8, 35, 42, 26, 34, 12, 2, 25, 13, 11, 32, 39, 31, 23, 18, 20, 22, 15, 41, 10, 27, 29, 5, 49, 48, 0, 3, 45, 1, 6, 21, 46, 33, 14]\n","epoch: 0, train_batch_id: 9299, avg_reward: 7.084596157073975\n","Example train output: [21, 23, 49, 0, 37, 29, 25, 8, 34, 12, 2, 18, 45, 38, 19, 14, 4, 44, 46, 1, 36, 42, 5, 15, 40, 20, 22, 43, 35, 26, 48, 6, 27, 28, 33, 3, 24, 32, 47, 7, 13, 16, 9, 41, 30, 17, 39, 31, 10, 11]\n","epoch: 0, train_batch_id: 9399, avg_reward: 7.050015449523926\n","Example train output: [41, 25, 14, 32, 20, 16, 45, 49, 15, 43, 38, 39, 12, 34, 44, 10, 1, 8, 36, 0, 47, 19, 21, 42, 22, 3, 40, 33, 13, 6, 26, 18, 24, 28, 29, 46, 7, 35, 11, 5, 37, 23, 4, 2, 9, 48, 31, 30, 17, 27]\n","epoch: 0, train_batch_id: 9499, avg_reward: 7.024662494659424\n","Example train output: [32, 19, 2, 15, 1, 37, 31, 36, 3, 18, 26, 43, 33, 17, 39, 4, 47, 24, 28, 42, 12, 22, 10, 41, 30, 38, 45, 0, 7, 48, 20, 40, 13, 11, 29, 25, 49, 9, 23, 8, 34, 6, 35, 16, 44, 21, 46, 27, 14, 5]\n","epoch: 0, train_batch_id: 9599, avg_reward: 7.188841342926025\n","Example train output: [7, 3, 4, 30, 27, 25, 20, 12, 16, 42, 31, 23, 17, 39, 8, 22, 45, 1, 2, 15, 14, 47, 44, 29, 11, 18, 41, 6, 24, 48, 49, 40, 36, 9, 32, 26, 10, 33, 0, 28, 37, 35, 34, 46, 5, 21, 38, 43, 19, 13]\n","epoch: 0, train_batch_id: 9699, avg_reward: 7.023584842681885\n","Example train output: [4, 44, 1, 49, 16, 41, 39, 7, 25, 14, 38, 5, 34, 20, 15, 26, 8, 23, 9, 32, 36, 10, 42, 11, 22, 48, 3, 6, 19, 24, 29, 45, 33, 17, 12, 43, 46, 18, 31, 30, 27, 13, 40, 37, 47, 0, 35, 2, 28, 21]\n","epoch: 0, train_batch_id: 9799, avg_reward: 7.027076721191406\n","Example train output: [15, 14, 16, 43, 7, 8, 36, 37, 30, 20, 40, 6, 10, 49, 4, 28, 22, 17, 2, 24, 12, 45, 25, 9, 47, 32, 44, 42, 41, 48, 34, 0, 1, 19, 29, 33, 18, 13, 38, 3, 11, 21, 23, 27, 35, 31, 5, 39, 46, 26]\n","epoch: 0, train_batch_id: 9899, avg_reward: 7.074728965759277\n","Example train output: [30, 33, 23, 0, 42, 2, 24, 3, 12, 15, 4, 48, 38, 18, 17, 32, 22, 35, 34, 27, 25, 1, 44, 36, 5, 43, 46, 40, 9, 16, 7, 20, 21, 13, 37, 26, 10, 41, 49, 8, 14, 45, 29, 28, 47, 6, 19, 11, 39, 31]\n","epoch: 0, train_batch_id: 9999, avg_reward: 7.008036136627197\n","Example train output: [19, 45, 20, 24, 4, 11, 8, 31, 40, 27, 5, 39, 25, 49, 46, 32, 48, 3, 21, 26, 15, 14, 36, 43, 38, 22, 42, 35, 16, 9, 44, 18, 0, 30, 33, 2, 17, 1, 6, 41, 34, 12, 28, 13, 23, 29, 7, 37, 47, 10]\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34692e8820a54fc38d314645f4a6537c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["  1%|▏         | 18151/1280000 [00:00<00:06, 181505.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Validation overall avg_reward: 6.663022041320801\n","Validation overall reward var: 0.002205916913226247\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1280000/1280000 [00:07<00:00, 165155.42it/s]\n"],"name":"stderr"}]},{"metadata":{"id":"FSl0QQA3j2kj","colab_type":"code","colab":{}},"cell_type":"code","source":["Reward = np.array(Reward)\n","print(Reward.shape)\n","k = np.arange(0, 50*Reward.shape[0], 50)\n","plt.plot(k, Reward)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UgPrb6ALj2h9","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"MPhcafUDj2fN","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"SeS2Dg2fxx1k","colab_type":"code","colab":{}},"cell_type":"code","source":["torch.save(model,'drive/My Drive/HRL-CO/model/TSP50.pt')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dIi-nkLC2VmS","colab_type":"code","colab":{}},"cell_type":"code","source":["model = torch.load('drive/My Drive/HRL-CO/model/TSP50.pt')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iZy0LvRr2lsH","colab_type":"code","colab":{}},"cell_type":"code","source":["# validation\n","example_tour = []\n","avg_reward = []\n","val_step = 0\n","model.eval()\n","model.actor_net.decoder.decode_type = \"greedy\"\n","    \n","for batch_id, val_batch in enumerate(tqdm_notebook(val_dataloader)):\n","    bat = Variable(val_batch)\n","\n","    if use_cuda:\n","        bat = bat.cuda()\n","\n","    R, probs, actions, action_idxs = model(bat)\n","    avg_reward.append(R.mean().cpu().numpy())\n","    val_step += 1.\n","\n","    if val_step % 5 == 0:\n","        example_output = []\n","        example_input = []\n","        for idx, action in enumerate(actions):\n","\n","            example_output.append(action_idxs[idx][0].item())\n","            # example_input.append(bat[0, :, idx].item())\n","            # example_tour.append(action.item())\n","        print('Step: {}'.format(batch_id))\n","        #print('Example test input: {}'.format(example_input))\n","        print('Example test output: {}'.format(example_output))\n","        print('Example test reward: {}'.format(R.mean()))\n","\n","print('Validation overall avg_reward: {}'.format(np.mean(avg_reward)))\n","print('Validation overall reward var: {}'.format(np.var(avg_reward)))"],"execution_count":0,"outputs":[]}]}